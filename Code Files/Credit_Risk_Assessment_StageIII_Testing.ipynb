{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d552e0a5",
   "metadata": {},
   "source": [
    "# Credit_Risk_Assessment Model Testing On New Data\n",
    "\n",
    "## Stage III\n",
    "\n",
    "\n",
    "   Predicting the Serious Delinquency in 2 years on new Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bbcb4",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b854d54c",
   "metadata": {},
   "source": [
    "### Setting SparkContext and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78bb5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Predicting on New Data\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ddf38",
   "metadata": {},
   "source": [
    "### Loading csv file into spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f036f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"file:///home/talentum/shared/CDAC_PROJECT/data/GiveMeSomeCredit-testing.csv\"\n",
    "\n",
    "df = spark.read.csv(file_path,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1b8a3",
   "metadata": {},
   "source": [
    "###  understand the data types and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47f40c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# Droping \"_c0\" column\n",
    "print(len(df.columns))\n",
    "df = df.drop('_c0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baca5d9",
   "metadata": {},
   "source": [
    "### Data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8dc28aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column  MonthlyIncome, NumberOfDependents have String Datatype.\n",
    "# Converting Datatype to Integer\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "df = df.withColumn(\"NumberOfDependents\", df[\"NumberOfDependents\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"MonthlyIncome\", df[\"MonthlyIncome\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58bb8db",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d7c896e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe have \"NA\" values\n",
    "# Replace 'NA' values with null\n",
    "df = df.replace('NA', None)\n",
    "\n",
    "# Detect missing values\n",
    "from pyspark.sql.functions import col, when, isnull, count\n",
    "missing_values = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6d9ba",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6971efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MonthlyIncome and Number of Dependents feature have null values \n",
    "# Impute missing values\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# 1. Impute MonthlyIncome with median\n",
    "median_income = df.approxQuantile(\"MonthlyIncome\", [0.5], 0.0)[0]\n",
    "df = df.withColumn(\"MonthlyIncome\", when(col(\"MonthlyIncome\").isNull(), median_income).otherwise(col(\"MonthlyIncome\")))\n",
    "\n",
    "# 2. Impute NumberOfDependents with mode\n",
    "mode_window = Window.partitionBy(\"NumberOfDependents\").orderBy(col(\"NumberOfDependents\").desc())\n",
    "mode_dependents = df.groupBy(\"NumberOfDependents\").count().orderBy(\"count\", ascending=False).first()[0]\n",
    "df = df.withColumn(\"NumberOfDependents\", when(col(\"NumberOfDependents\").isNull(), mode_dependents).otherwise(col(\"NumberOfDependents\")))\n",
    "\n",
    "# Display the cleaned and transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d6f52",
   "metadata": {},
   "source": [
    "### Outlier detection and handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ca81b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, when, round\n",
    "\n",
    "\n",
    "# Function to cap outliers using IQR\n",
    "def cap_outliers(col_name, df):\n",
    "    quantiles = df.approxQuantile(col_name, [0.25, 0.75], 0.05)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df = df.withColumn(col_name, when(col(col_name) < lower_bound, lower_bound)\n",
    "                                    .when(col(col_name) > upper_bound, upper_bound)\n",
    "                                    .otherwise(col(col_name)))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Drop records where age is less than 21\n",
    "df = df.filter(col(\"age\") >= 21)\n",
    "\n",
    "# Applying capping for columns with potential outliers using IQR\n",
    "iqr_columns = [\"age\", \"DebtRatio\", \"MonthlyIncome\", \"RevolvingUtilizationOfUnsecuredLines\"]\n",
    "for col_name in iqr_columns:\n",
    "    df = cap_outliers(col_name, df)\n",
    "\n",
    "# Round off the values after applying IQR to the age column and convert to integer\n",
    "df = df.withColumn(\"age\", round(col(\"age\"), 0).cast(\"integer\"))\n",
    "\n",
    "# Handle outliers by dropping records with values greater than specified thresholds\n",
    "df = df.filter(col(\"NumberOfTimes90DaysLate\") <= 8)\n",
    "df = df.filter(col(\"NumberOfTime60-89DaysPastDueNotWorse\") <= 12)\n",
    "df = df.filter(col(\"NumberOfTime30-59DaysPastDueNotWorse\") <= 24)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d513b",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d29f2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'DebtRatioCategory'\n",
    "\n",
    "df = df.withColumn(\"DebtRatioCategory\", when(col(\"DebtRatio\") < 0.2, \"Low\")\n",
    "                                       .when(col(\"DebtRatio\") < 0.5, \"Medium\")\n",
    "                                       .otherwise(\"High\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50722d0c",
   "metadata": {},
   "source": [
    "# Loading Saved Model and Giving Prediction on New Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d00f75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n",
      "|SeriousDlqin2yrs|MonthlyIncome|\n",
      "+----------------+-------------+\n",
      "|             0.0|       3041.0|\n",
      "|             1.0|       5400.0|\n",
      "|             1.0|       5400.0|\n",
      "|             0.0|       4955.0|\n",
      "|             1.0|       2500.0|\n",
      "|             0.0|       5400.0|\n",
      "|             1.0|       7625.0|\n",
      "|             1.0|      11376.0|\n",
      "|             0.0|       2950.0|\n",
      "|             0.0|       5400.0|\n",
      "+----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- SeriousDlqin2yrs: double (nullable = false)\n",
      " |-- RevolvingUtilizationOfUnsecuredLines: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- NumberOfTime30-59DaysPastDueNotWorse: double (nullable = true)\n",
      " |-- DebtRatio: double (nullable = true)\n",
      " |-- MonthlyIncome: double (nullable = true)\n",
      " |-- NumberOfOpenCreditLinesAndLoans: integer (nullable = true)\n",
      " |-- NumberOfTimes90DaysLate: integer (nullable = true)\n",
      " |-- NumberRealEstateLoansOrLines: integer (nullable = true)\n",
      " |-- NumberOfTime60-89DaysPastDueNotWorse: integer (nullable = true)\n",
      " |-- NumberOfDependents: integer (nullable = true)\n",
      " |-- DebtRatioCategory: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Load the saved model from HDFS\n",
    "model_path = \"hdfs:///user/talentum/models/\"\n",
    "model = PipelineModel.load(model_path)\n",
    "\n",
    "# Predict the label column 'SeriousDlqin2yrs'\n",
    "df_predictions = model.transform(df)\n",
    "df_result = df_predictions.withColumn(\"SeriousDlqin2yrs\", col(\"prediction\"))\n",
    "df_result = df_result.drop(\"prediction\",\"features\",\"rawPrediction\",\"probability\",\"debtratiocategory_index\",\"debtratiocategory_vec\")\n",
    "df_result.select(\"SeriousDlqin2yrs\", \"MonthlyIncome\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a7718",
   "metadata": {},
   "source": [
    "# Saving Predicted Dataset Into hdfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf04daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory in HDFS\n",
    "hdfs_output_dir = \"hdfs:///user/talentum/Prediction_data/\"\n",
    "\n",
    "# Save the DataFrame to HDFS in overwrite mode\n",
    "df_result.coalesce(1).write.mode(\"overwrite\").csv(hdfs_output_dir, header=True)\n",
    "\n",
    "# hdfs dfs -getmerge Prediction_data/ ~/shared/CDAC_PROJECT/Prediction_data/Prediction_data.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bcff0aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|SeriousDlqin2yrs|count|\n",
      "+----------------+-----+\n",
      "|             0.0|98143|\n",
      "|             1.0| 3043|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.groupBy(\"SeriousDlqin2yrs\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
