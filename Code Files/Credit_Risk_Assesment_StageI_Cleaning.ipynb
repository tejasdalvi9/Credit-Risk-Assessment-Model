{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d552e0a5",
   "metadata": {},
   "source": [
    "# Credit Risk Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bbcb4",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b854d54c",
   "metadata": {},
   "source": [
    "### Setting SparkContext and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78bb5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataSet Cleaning and Preprocessing\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ddf38",
   "metadata": {},
   "source": [
    "### Loading csv file into spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f036f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"file:////home/talentum/shared/Project/project/GiveMeSomeCredit-training.csv\"\n",
    "\n",
    "df = spark.read.csv(file_path,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1b8a3",
   "metadata": {},
   "source": [
    "###  understand the data types and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6f8488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Records :  150000\n",
      "Number of Columns :  12\n"
     ]
    }
   ],
   "source": [
    "# Shape of Data\n",
    "print(\"Number of Records : \",df.count())\n",
    "print(\"Number of Columns : \", len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a4444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- SeriousDlqin2yrs: integer (nullable = true)\n",
      " |-- RevolvingUtilizationOfUnsecuredLines: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- NumberOfTime30-59DaysPastDueNotWorse: integer (nullable = true)\n",
      " |-- DebtRatio: double (nullable = true)\n",
      " |-- MonthlyIncome: string (nullable = true)\n",
      " |-- NumberOfOpenCreditLinesAndLoans: integer (nullable = true)\n",
      " |-- NumberOfTimes90DaysLate: integer (nullable = true)\n",
      " |-- NumberRealEstateLoansOrLines: integer (nullable = true)\n",
      " |-- NumberOfTime60-89DaysPastDueNotWorse: integer (nullable = true)\n",
      " |-- NumberOfDependents: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Schema of Data\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47f40c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Droping \"_c0\" column\n",
    "print(len(df.columns))\n",
    "df = df.drop('_c0')\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baca5d9",
   "metadata": {},
   "source": [
    "### Data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc28aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|MonthlyIncome|\n",
      "+-------------+\n",
      "|         9120|\n",
      "|         2600|\n",
      "|         3042|\n",
      "|         3300|\n",
      "|        63588|\n",
      "|         3500|\n",
      "|           NA|\n",
      "|         3500|\n",
      "|           NA|\n",
      "|        23684|\n",
      "+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "+------------------+\n",
      "|NumberOfDependents|\n",
      "+------------------+\n",
      "|                 2|\n",
      "|                 1|\n",
      "|                 0|\n",
      "|                 0|\n",
      "|                 0|\n",
      "|                 1|\n",
      "|                 0|\n",
      "|                 0|\n",
      "|                NA|\n",
      "|                 2|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- SeriousDlqin2yrs: integer (nullable = true)\n",
      " |-- RevolvingUtilizationOfUnsecuredLines: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- NumberOfTime30-59DaysPastDueNotWorse: integer (nullable = true)\n",
      " |-- DebtRatio: double (nullable = true)\n",
      " |-- MonthlyIncome: double (nullable = true)\n",
      " |-- NumberOfOpenCreditLinesAndLoans: integer (nullable = true)\n",
      " |-- NumberOfTimes90DaysLate: integer (nullable = true)\n",
      " |-- NumberRealEstateLoansOrLines: integer (nullable = true)\n",
      " |-- NumberOfTime60-89DaysPastDueNotWorse: integer (nullable = true)\n",
      " |-- NumberOfDependents: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column  MonthlyIncome, NumberOfDependents have String Datatype.\n",
    "# Converting Datatype to Integer\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "print(df.select(df.MonthlyIncome).show(10))\n",
    "print(df.select(df.NumberOfDependents).show(10))\n",
    "\n",
    "df = df.withColumn(\"NumberOfDependents\", df[\"NumberOfDependents\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"MonthlyIncome\", df[\"MonthlyIncome\"].cast(DoubleType()))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58bb8db",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c896e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------------------------+---+------------------------------------+---------+\n",
      "|SeriousDlqin2yrs|RevolvingUtilizationOfUnsecuredLines|age|NumberOfTime30-59DaysPastDueNotWorse|DebtRatio|\n",
      "+----------------+------------------------------------+---+------------------------------------+---------+\n",
      "|               0|                                   0|  0|                                   0|        0|\n",
      "+----------------+------------------------------------+---+------------------------------------+---------+\n",
      "\n",
      "+-------------+-------------------------------+-----------------------+----------------------------+\n",
      "|MonthlyIncome|NumberOfOpenCreditLinesAndLoans|NumberOfTimes90DaysLate|NumberRealEstateLoansOrLines|\n",
      "+-------------+-------------------------------+-----------------------+----------------------------+\n",
      "|        29731|                              0|                      0|                           0|\n",
      "+-------------+-------------------------------+-----------------------+----------------------------+\n",
      "\n",
      "+------------------------------------+------------------+\n",
      "|NumberOfTime60-89DaysPastDueNotWorse|NumberOfDependents|\n",
      "+------------------------------------+------------------+\n",
      "|                                   0|              3924|\n",
      "+------------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataframe have \"NA\" values\n",
    "# Replace 'NA' values with null\n",
    "df = df.replace('NA', None)\n",
    "\n",
    "# Detect missing values\n",
    "from pyspark.sql.functions import col, when, isnull, count\n",
    "missing_values = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns])\n",
    "missing_values.select(missing_values.columns[0:5]).show()\n",
    "missing_values.select(missing_values.columns[5:9]).show()\n",
    "missing_values.select(missing_values.columns[9:12]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6d9ba",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6971efbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median MonthlyIncome: 5400.0\n",
      "Mode NumberOfDependents: 0\n",
      "+-------------+------------------+\n",
      "|MonthlyIncome|NumberOfDependents|\n",
      "+-------------+------------------+\n",
      "|       9120.0|                 2|\n",
      "|       2600.0|                 1|\n",
      "|       3042.0|                 0|\n",
      "|       3300.0|                 0|\n",
      "|      63588.0|                 0|\n",
      "|       3500.0|                 1|\n",
      "|       5400.0|                 0|\n",
      "|       3500.0|                 0|\n",
      "|       5400.0|                 0|\n",
      "|      23684.0|                 2|\n",
      "+-------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MonthlyIncome and Number of Dependents feature have null values \n",
    "# Impute missing values\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# 1. Impute MonthlyIncome with median\n",
    "median_income = df.approxQuantile(\"MonthlyIncome\", [0.5], 0.0)[0]\n",
    "print(f\"Median MonthlyIncome: {median_income}\")\n",
    "df = df.withColumn(\"MonthlyIncome\", when(col(\"MonthlyIncome\").isNull(), median_income).otherwise(col(\"MonthlyIncome\")))\n",
    "\n",
    "# 2. Impute NumberOfDependents with mode\n",
    "mode_window = Window.partitionBy(\"NumberOfDependents\").orderBy(col(\"NumberOfDependents\").desc())\n",
    "mode_dependents = df.groupBy(\"NumberOfDependents\").count().orderBy(\"count\", ascending=False).first()[0]\n",
    "df = df.withColumn(\"NumberOfDependents\", when(col(\"NumberOfDependents\").isNull(), mode_dependents).otherwise(col(\"NumberOfDependents\")))\n",
    "print(f\"Mode NumberOfDependents: {mode_dependents}\")\n",
    "\n",
    "# Display the cleaned and transformed data\n",
    "df.select('MonthlyIncome','NumberOfDependents').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ecca26",
   "metadata": {},
   "source": [
    "### Summary of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b0cb5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------------------------+------------------+------------------------------------+\n",
      "|summary|   SeriousDlqin2yrs|RevolvingUtilizationOfUnsecuredLines|               age|NumberOfTime30-59DaysPastDueNotWorse|\n",
      "+-------+-------------------+------------------------------------+------------------+------------------------------------+\n",
      "|  count|             150000|                              150000|            150000|                              150000|\n",
      "|   mean|            0.06684|                   6.048438054666792|52.295206666666665|                  0.4210333333333333|\n",
      "| stddev|0.24974553092871946|                  249.75537062544046|14.771865863100334|                    4.19278127201834|\n",
      "|    min|                  0|                                 0.0|                 0|                                   0|\n",
      "|    25%|                  0|                         0.029859118|                41|                                   0|\n",
      "|    50%|                  0|                         0.154170481|                52|                                   0|\n",
      "|    75%|                  0|                         0.558978398|                63|                                   0|\n",
      "|    max|                  1|                             50708.0|               109|                                  98|\n",
      "+-------+-------------------+------------------------------------+------------------+------------------------------------+\n",
      "\n",
      "+-------+-----------------+------------------+-------------------------------+-----------------------+\n",
      "|summary|        DebtRatio|     MonthlyIncome|NumberOfOpenCreditLinesAndLoans|NumberOfTimes90DaysLate|\n",
      "+-------+-----------------+------------------+-------------------------------+-----------------------+\n",
      "|  count|           150000|            150000|                         150000|                 150000|\n",
      "|   mean|353.0050757638713|        6418.45492|                        8.45276|    0.26597333333333334|\n",
      "| stddev|2037.818523144368|12890.395542154793|              5.145950989643277|      4.169303787594446|\n",
      "|    min|              0.0|               0.0|                              0|                      0|\n",
      "|    25%|      0.175069472|            3904.0|                              5|                      0|\n",
      "|    50%|      0.366472403|            5400.0|                              8|                      0|\n",
      "|    75%|      0.868131868|            7400.0|                             11|                      0|\n",
      "|    max|         329664.0|         3008750.0|                             58|                     98|\n",
      "+-------+-----------------+------------------+-------------------------------+-----------------------+\n",
      "\n",
      "+-------+----------------------------+------------------------------------+------------------+\n",
      "|summary|NumberRealEstateLoansOrLines|NumberOfTime60-89DaysPastDueNotWorse|NumberOfDependents|\n",
      "+-------+----------------------------+------------------------------------+------------------+\n",
      "|  count|                      150000|                              150000|            150000|\n",
      "|   mean|                     1.01824|                 0.24038666666666667|0.7374133333333334|\n",
      "| stddev|          1.1297709848828508|                   4.155179420987239|1.1070214146370245|\n",
      "|    min|                           0|                                   0|                 0|\n",
      "|    25%|                           0|                                   0|                 0|\n",
      "|    50%|                           1|                                   0|                 0|\n",
      "|    75%|                           2|                                   0|                 1|\n",
      "|    max|                          54|                                  98|                20|\n",
      "+-------+----------------------------+------------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.columns[0:4]).summary().show()\n",
    "df.select(df.columns[4:8]).summary().show()\n",
    "df.select(df.columns[8:12]).summary().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41c28c",
   "metadata": {},
   "source": [
    "### Storing Dataframe With Outliers (Cleaned) into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b46e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory in HDFS\n",
    "hdfs_output_dir = \"hdfs:///user/talentum/processed_data/cleaned_data_with_outliers\"\n",
    "\n",
    "# Save the DataFrame to HDFS in overwrite mode\n",
    "df.coalesce(1).write.mode(\"overwrite\").csv(hdfs_output_dir, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fadfa",
   "metadata": {},
   "source": [
    "### Storing Dataframe With Outliers (Cleaned) into HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc400174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|    SeriousDlqin2yrs|      int|   null|\n",
      "|RevolvingUtilizat...|   double|   null|\n",
      "|                 age|      int|   null|\n",
      "|NumberOfTime30-59...|      int|   null|\n",
      "|           DebtRatio|   double|   null|\n",
      "|       MonthlyIncome|   double|   null|\n",
      "|NumberOfOpenCredi...|      int|   null|\n",
      "|NumberOfTimes90Da...|      int|   null|\n",
      "|NumberRealEstateL...|      int|   null|\n",
      "|NumberOfTime60-89...|      int|   null|\n",
      "|  NumberOfDependents|      int|   null|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"cleaned_data_with_outliers\")\n",
    "\n",
    "# Query the Hive table to verify\n",
    "spark.sql(\"describe cleaned_data_with_outliers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d6f52",
   "metadata": {},
   "source": [
    "### Outlier detection and handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca81b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, when, round\n",
    "\n",
    "\n",
    "# Function to cap outliers using IQR\n",
    "def cap_outliers(col_name, df):\n",
    "    quantiles = df.approxQuantile(col_name, [0.25, 0.75], 0.05)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df = df.withColumn(col_name, when(col(col_name) < lower_bound, lower_bound)\n",
    "                                    .when(col(col_name) > upper_bound, upper_bound)\n",
    "                                    .otherwise(col(col_name)))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Drop records where age is less than 21\n",
    "df = df.filter(col(\"age\") >= 21)\n",
    "\n",
    "# Applying capping for columns with potential outliers using IQR\n",
    "iqr_columns = [\"age\", \"DebtRatio\", \"MonthlyIncome\", \"RevolvingUtilizationOfUnsecuredLines\"]\n",
    "for col_name in iqr_columns:\n",
    "    df = cap_outliers(col_name, df)\n",
    "\n",
    "# Round off the values after applying IQR to the age column and convert to integer\n",
    "df = df.withColumn(\"age\", round(col(\"age\"), 0).cast(\"integer\"))\n",
    "\n",
    "# Handle outliers by dropping records with values greater than specified thresholds\n",
    "df = df.filter(col(\"NumberOfTimes90DaysLate\") <= 8)\n",
    "df = df.filter(col(\"NumberOfTime60-89DaysPastDueNotWorse\") <= 12)\n",
    "df = df.filter(col(\"NumberOfTime30-59DaysPastDueNotWorse\") <= 24)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.columns[0:4]).summary().show()\n",
    "df.select(df.columns[4:8]).summary().show()\n",
    "df.select(df.columns[8:12]).summary().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d513b",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'DebtRatioCategory'\n",
    "\n",
    "df = df.withColumn(\"DebtRatioCategory\", when(col(\"DebtRatio\") < 0.2, \"Low\")\n",
    "                                       .when(col(\"DebtRatio\") < 0.5, \"Medium\")\n",
    "                                       .otherwise(\"High\"))\n",
    "\n",
    "print(\"Number of Columns : \",len(df.columns))\n",
    "df.select('DebtRatioCategory').printSchema()\n",
    "df.select('DebtRatioCategory').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee77d3a",
   "metadata": {},
   "source": [
    "### Storing  DateFrame WITHOUT OUTLIERS In HDFS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory in HDFS\n",
    "hdfs_output_dir = \"hdfs:///user/talentum/processed_data/cleaned_data_without_outliers\"\n",
    "\n",
    "# Save the DataFrame to HDFS in overwrite mode\n",
    "df.coalesce(1).write.mode(\"overwrite\").csv(hdfs_output_dir, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b7ea76",
   "metadata": {},
   "source": [
    "### Storing  DateFrame WITHOUT OUTLIERS In Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"cleaned_data_without_outliers\")\n",
    "\n",
    "# Query the Hive table to verify\n",
    "spark.sql(\"describe cleaned_data_without_outliers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709b1623",
   "metadata": {},
   "source": [
    "### Normalization/Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1673ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization/Scaling (example: Min-Max Scaling for numerical columns)\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# List of numerical columns to scale\n",
    "numerical_cols = [\"RevolvingUtilizationOfUnsecuredLines\", \"age\", \"DebtRatio\", \"MonthlyIncome\", \"NumberOfOpenCreditLinesAndLoans\"]\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(df)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "scaled_df = scaler_model.transform(assembled_df)\n",
    "\n",
    "# Display the cleaned and transformed data\n",
    "scaled_df.select(scaled_df.columns[0:5]).show(5)\n",
    "scaled_df.select(scaled_df.columns[5:9]).show(5)\n",
    "scaled_df.select(scaled_df.columns[9:12]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb2ec96",
   "metadata": {},
   "source": [
    "### Storing  DateFrame SCALLED  In HDFS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce73355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory in HDFS\n",
    "hdfs_output_dir = \"hdfs:///user/talentum/processed_data/cleaned_data_scalled\"\n",
    "\n",
    "# Save the DataFrame to HDFS in overwrite mode\n",
    "df.coalesce(1).write.mode(\"overwrite\").csv(hdfs_output_dir, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b454e6",
   "metadata": {},
   "source": [
    "### Storing  DateFrame WITHOUT OUTLIERS In Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05be0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"cleaned_data_scalled\")\n",
    "\n",
    "# Query the Hive table to verify\n",
    "spark.sql(\"describe cleaned_data_scalled\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023ecaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
